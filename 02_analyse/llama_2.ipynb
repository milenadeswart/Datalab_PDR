{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dit is oude, niet-werkende code die geen onderdeel vormt van de pipeline.\n",
    "### This is old code that is not-working and not a part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_from_json(json_file_path):\n",
    "    \"\"\"\n",
    "    Reads a dictionary from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    json_file_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary read from the JSON file, or None if an error occurred.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"Error: The file {json_file_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            return data\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {json_file_path} does not contain valid JSON.\")\n",
    "  \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_axiom_sentence = read_dict_from_json('/data4T/jieying/data/ontologies/GeoFault/merge_geofault_xml_del_pt.owl_id_to_axiom_sentence.json')\n",
    "# id_to_axiom_sentence = read_dict_from_json('/data4T/jieying/data/ontologies/disease/ontology-2023-09-05_16-12-34.owl_branch.clinical_finding.disease.owl_id_to_axiom_sentence.json')\n",
    "\n",
    "# file_path = '/data4T/jieying/data/ontologies/GeoFault/GeoFaultBenchmark.pkl_finalAnalysedResult.pkl'\n",
    "# file_path = '/data4T/jieying/data/ontologies/foodon/benchmark/achive/benchmark_raw_grouped_manual_part_1.csv_withoutFAISS_finalAnalysedResult.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Insufficient arguments provided. Please provide the model name, JSON file path, data file path, and GPU ID.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ca4d2d8a3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Insufficient arguments provided. Please provide the model name, JSON file path, data file path, and GPU ID.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model_7b = sys.argv[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Insufficient arguments provided. Please provide the model name, JSON file path, data file path, and GPU ID."
     ]
    }
   ],
   "source": [
    "# Get command line arguments\n",
    "model_7b = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "if len(sys.argv) < 4:\n",
    "    raise ValueError(\"Insufficient arguments provided. Please provide the model name, JSON file path, data file path, and GPU ID.\")\n",
    "\n",
    "# model_7b = sys.argv[1]\n",
    "json_file_path = sys.argv[1]\n",
    "file_path = sys.argv[2]\n",
    "gpu_id = sys.argv[3]\n",
    "\n",
    "# Read data\n",
    "id_to_axiom_sentence = read_dict_from_json(json_file_path)\n",
    "\n",
    "# Keeping only the first two rows\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the prompt for each row\n",
    "# def generate_prompt_for_row(query, candidate_ids, id_to_sentence_map):\n",
    "#     # Start the prompt with the query from the DataFrame\n",
    "#     prompt = \"### Input:\"\n",
    "#     prompt += \"Could you please find the most relevant setences from the following candidate sentences with respect to the reference sentence. Please provide the ranking in the format: \\\"The ranking of candidates is: [ranked list of IDs].Please answer briefly using candidate IDs, separated by commas.\\\"\\n\\n\"\n",
    "#     prompt += f\"Here is the reference sentence:\\n\\\"{query}\\\"\\n\\n\"\n",
    "#     prompt += \"Candidates:\\n\\n\"\n",
    "\n",
    "#     for id in candidate_ids:\n",
    "#         sentence = id_to_sentence_map.get(id, \"Sentence not found for ID {}\".format(id))\n",
    "#         new_line = f\"ID: {id} - \\\"{sentence}\\\"\\n\"\n",
    "#         if len(prompt) + len(new_line) < 2024:  # Check to keep under token limit\n",
    "#             prompt += new_line\n",
    "#         else:\n",
    "#             break  # Stop adding more candidates if limit is reached\n",
    "\n",
    "#     prompt += \"### Response:\"\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_for_row(query, candidate_ids, id_to_sentence_map, k):\n",
    "    # Start the prompt with the query from the DataFrame\n",
    "    prompt = \"### Input:\"\n",
    "    prompt += \"Could you please find the most relevant setences from the following candidate sentences with respect to the reference sentence. Please provide the ranking in the format: \\\"The ranking of candidates is: [ranked list of IDs].Please answer briefly using candidate IDs, separated by commas.\\\"\\n\\n\"\n",
    "    prompt += f\"Here is the reference sentence:\\n\\\"{query}\\\"\\n\\n\"\n",
    "    prompt += \"Candidates:\\n\\n\"\n",
    "\n",
    "    # Include up to 'k' candidate IDs in the prompt\n",
    "    for id in candidate_ids[:k]:  # Only iterate over the first 'k' IDs\n",
    "        sentence = id_to_sentence_map.get(id, \"Sentence not found for ID {}\".format(id))\n",
    "        prompt += f\"ID: {id} - \\\"{sentence}\\\"\\n\"\n",
    "\n",
    "    prompt += \"### Response:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "# df['SBERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['SBERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "# df['BERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['BERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "meta-llama/Meta-Llama-3-8B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     )\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    276\u001b[0m             )\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6650531f-31f3e1f17caace18267a1018;df05f8cc-17d1-4832-95ba-32f07487acc5)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/tokenizer_config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must be authenticated to access it.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75cf827b3bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_7b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Meta-Llama-3-8B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer_7b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_7b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Specify the device explicitly in the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0m_commit_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         raise EnvironmentError(\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;34m\"pass a token having permission to this repo with `use_auth_token` or log in with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: meta-llama/Meta-Llama-3-8B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and specify GPU ID\n",
    "device = f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_7b = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer_7b = AutoTokenizer.from_pretrained(model_7b)\n",
    "\n",
    "# Specify the device explicitly in the pipeline\n",
    "pipeline_7b = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_7b,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response and measure computation time\n",
    "def generate_response(query):\n",
    "    start_time = time.time()\n",
    "\n",
    "    llama_prompt = \"[INST]\" + query + \"[/INST]\"\n",
    "    with torch.no_grad():\n",
    "        sequences = pipeline_7b(\n",
    "            llama_prompt,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer_7b.eos_token_id,\n",
    "            max_length=9000,\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    computation_time = end_time - start_time\n",
    "\n",
    "    # print(sequences[0]['generated_text'])\n",
    "    return sequences[0]['generated_text'], computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and record results\n",
    "final_output_file = file_path + \"_llama7b.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_queries_in_batches(listA, batch_size=10):\n",
    "#     results = []\n",
    "#     for batch_start in tqdm(range(0, len(listA), batch_size), desc=\"Processing batches\"):\n",
    "#         batch_end = min(batch_start + batch_size, len(listA))\n",
    "#         queries = listA[batch_start:batch_end]\n",
    "\n",
    "#         # Process each query and record the time\n",
    "#         batch_results = []\n",
    "#         for query in queries:\n",
    "#             start_time = time.time()\n",
    "#             response = generate_response(query)\n",
    "#             end_time = time.time()\n",
    "#             time_taken = end_time - start_time\n",
    "#             batch_results.append((response, time_taken))\n",
    "\n",
    "#         results.extend(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate prompts\n",
    "# ks = [10, 20, 50]\n",
    "\n",
    "# for k in ks:\n",
    "#     df[f'SBERT Prompt {k}'] = df.apply(\n",
    "#         lambda row: generate_prompt_for_row(\n",
    "#             row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "#             list(row['SBERT_Ranking'].keys()),\n",
    "#             id_to_axiom_sentence, k\n",
    "#         ),\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     df[f'BERT Prompt {k}'] = df.apply(\n",
    "#         lambda row: generate_prompt_for_row(\n",
    "#             row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "#             list(row['BERT_Ranking'].keys()),\n",
    "#             id_to_axiom_sentence, k\n",
    "#         ),\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     df[f'SapBERT Prompt {k}'] = df.apply(\n",
    "#         lambda row: generate_prompt_for_row(\n",
    "#             row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "#             list(row['SapBERT_Ranking'].keys()),\n",
    "#             id_to_axiom_sentence, k\n",
    "#         ),\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "# df['SBERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['SBERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "# df['BERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['BERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "# df['SapBERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['SapBERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries_in_chunks(df, query_column, answer_column, final_output_file, chunk_size=100, batch_size=10, save_every=20):\n",
    "    for chunk_start in range(0, len(df), chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(df))\n",
    "        chunk = df.iloc[chunk_start:chunk_end]\n",
    "\n",
    "        for batch_start in range(0, len(chunk), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(chunk))\n",
    "            queries = chunk[query_column][batch_start:batch_end].tolist()\n",
    "\n",
    "            batch_results = []\n",
    "\n",
    "            for i, query in enumerate(queries):\n",
    "                if chunk.iloc[i][answer_column] == 'tba':\n",
    "                    response = generate_response(query)\n",
    "                else:\n",
    "                    response = chunk.iloc[i][answer_column]\n",
    "                batch_results.append(response)\n",
    "\n",
    "            for i, result in enumerate(batch_results):\n",
    "                if(chunk_start + batch_start + 1<len(df)):\n",
    "                    df[answer_column].iloc[chunk_start + batch_start + i] = result\n",
    "\n",
    "            if (chunk_start + batch_start) % save_every == 0:\n",
    "                df.to_pickle(final_output_file)\n",
    "\n",
    "    df.to_pickle(final_output_file)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_queries_in_chunks(df,'BERT Prompt','Answer_BERT_llama_7b',final_output_file)\n",
    "# process_queries_in_chunks(df,'SBERT Prompt','Answer_SBERT_llama_7b',final_output_file)\n",
    "# process_queries_in_chunks(df,'SapBERT Prompt','Answer_SapBERT_llama_7b',final_output_file)\n",
    "\n",
    "ks = [10, 20]\n",
    "\n",
    "for k in ks:\n",
    "    # Generate prompts for each k\n",
    "    df[f'SBERT Prompt {k}'] = df.apply(\n",
    "        lambda row: generate_prompt_for_row(\n",
    "            row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "            list(row['SBERT_Ranking'].keys()),\n",
    "            id_to_axiom_sentence, k\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[f'BERT Prompt {k}'] = df.apply(\n",
    "        lambda row: generate_prompt_for_row(\n",
    "            row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "            list(row['BERT_Ranking'].keys()),\n",
    "            id_to_axiom_sentence, k\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df[f'SapBERT Prompt {k}'] = df.apply(\n",
    "        lambda row: generate_prompt_for_row(\n",
    "            row['Annotation Text'] if 'Annotation Text' in df.columns else row['Query'],\n",
    "            list(row['SapBERT_Ranking'].keys()),\n",
    "            id_to_axiom_sentence, k\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Check if each column exists, and if not, create it with default value 'tba'\n",
    "    if f'Answer_BERT_llama_7b_{k}' not in df.columns:\n",
    "        df[f'Answer_BERT_llama_7b_{k}'] = 'tba'\n",
    "\n",
    "    if f'Answer_SBERT_llama_7b_{k}' not in df.columns:\n",
    "        df[f'Answer_SBERT_llama_7b_{k}'] = 'tba'\n",
    "\n",
    "    if f'Answer_SapBERT_llama_7b_{k}' not in df.columns:\n",
    "        df[f'Answer_SapBERT_llama_7b_{k}'] = 'tba'\n",
    "\n",
    "    # Process the generated prompts\n",
    "    process_queries_in_chunks(df, f'BERT Prompt {k}', f'Answer_BERT_llama_7b_{k}', final_output_file)\n",
    "    df.drop(f'BERT Prompt {k}', axis=1, inplace=True)\n",
    "    process_queries_in_chunks(df, f'SBERT Prompt {k}', f'Answer_SBERT_llama_7b_{k}', final_output_file)\n",
    "    df.drop(f'SBERT Prompt {k}', axis=1, inplace=True)\n",
    "    process_queries_in_chunks(df, f'SapBERT Prompt {k}', f'Answer_SapBERT_llama_7b_{k}', final_output_file)s\n",
    "    df.drop(f'SapBERT Prompt {k}', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_results = process_queries_in_batches(df['BERT Prompt'])\n",
    "# df['Answer_BERT_llama_7b'], df['Time_BERT_prompt_llama2_7b'] = zip(*batched_results)\n",
    "\n",
    "# sbert_batched_results = process_queries_in_batches(df['SBERT Prompt'])\n",
    "# df['Answer_SBERT_llama_7b'], df['Time_SBERT_prompt_llama2_7b'] = zip(*sbert_batched_results)\n",
    "\n",
    "# sapbert_batched_results = process_queries_in_batches(df['SapBERT Prompt'])\n",
    "# df['Answer_SapBERT_llama_7b'], df['Time_SapBERT_prompt_llama2_7b'] = zip(*sapbert_batched_results)\n",
    "\n",
    "df.to_pickle(final_output_file)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, pipeline\n",
    "# import transformers\n",
    "# import torch\n",
    "# from tqdm.auto import tqdm\n",
    "# import time\n",
    "# import os\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_dict_from_json(json_file_path):\n",
    "#     \"\"\"\n",
    "#     Reads a dictionary from a JSON file.\n",
    "\n",
    "#     Parameters:\n",
    "#     json_file_path (str): Path to the JSON file.\n",
    "\n",
    "#     Returns:\n",
    "#     dict: Dictionary read from the JSON file, or None if an error occurred.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Check if the file exists\n",
    "#     if not os.path.exists(json_file_path):\n",
    "#         print(f\"Error: The file {json_file_path} does not exist.\")\n",
    "#         return None\n",
    "\n",
    "#     try:\n",
    "#         # Open and read the JSON file\n",
    "#         with open(json_file_path, 'r') as file:\n",
    "#             data = json.load(file)\n",
    "#             return data\n",
    "\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(f\"Error: The file {json_file_path} does not contain valid JSON.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_7b = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# # id_to_axiom_sentence = read_dict_from_json('/data4T/jieying/data/ontologies/GeoFault/merge_geofault_xml_del_pt.owl_id_to_axiom_sentence.json')\n",
    "# id_to_axiom_sentence = read_dict_from_json('/data4T/jieying/data/ontologies/foodon/foodon-merged.owl_id_to_axiom_sentence.json')\n",
    "\n",
    "# # file_path = '/data4T/jieying/data/ontologies/GeoFault/GeoFaultBenchmark.pkl_finalAnalysedResult.pkl'\n",
    "# file_path = '/data4T/jieying/data/ontologies/foodon/benchmark/achive/benchmark_raw_grouped_manual_part_1.csv_withoutFAISS_finalAnalysedResult.pkl'\n",
    "\n",
    "# # Keeping only the first two rows\n",
    "# df = pd.read_pickle(file_path)\n",
    "\n",
    "# # Update the pipeline creation\n",
    "# device = 0 if torch.cuda.is_available() else -1  # GPU if available, otherwise CPU\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_7b).to(device)\n",
    "# tokenizer_7b = AutoTokenizer.from_pretrained(model_7b)\n",
    "# pipeline_7b = TextGenerationPipeline(model=model, tokenizer=tokenizer_7b, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate the prompt for each row\n",
    "# def generate_prompt_for_row(query, candidate_ids, id_to_sentence_map):\n",
    "#     # Start the prompt with the query from the DataFrame\n",
    "#     prompt = \"### Input:\"\n",
    "#     prompt += \"Could you please find the most relevant setences from the following candidate sentences with respect to the reference sentence. Please provide the ranking in the format: \\\"The ranking of candidates is: [ranked list of IDs].Please answer briefly using candidate IDs, separated by commas.\\\"\\n\\n\"\n",
    "#     prompt += f\"Here is the reference sentence:\\n\\\"{query}\\\"\\n\\n\"\n",
    "#     prompt += \"Candidates:\\n\\n\"\n",
    "\n",
    "#     for id in candidate_ids:\n",
    "#         sentence = id_to_sentence_map.get(id, \"Sentence not found for ID {}\".format(id))\n",
    "#         new_line = f\"ID: {id} - \\\"{sentence}\\\"\\n\"\n",
    "#         if len(prompt) + len(new_line) < 2024:  # Check to keep under token limit\n",
    "#             prompt += new_line\n",
    "#         else:\n",
    "#             break  # Stop adding more candidates if limit is reached\n",
    "\n",
    "#     prompt += \"### Response:\"\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function to each row in the DataFrame\n",
    "# # df['SBERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['SBERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "# # df['BERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Query'], row['BERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate response and measure computation time\n",
    "# def generate_response(query):\n",
    "#     start_time = time.time()\n",
    "#     llama_prompt = \"[INST]\" + query + \"[/INST]\"\n",
    "#     sequences = pipeline_7b(\n",
    "#         llama_prompt,\n",
    "#         do_sample=True,\n",
    "#         top_k=10,\n",
    "#         num_return_sequences=1,\n",
    "#         eos_token_id=tokenizer_7b.eos_token_id,\n",
    "#         max_length=4000,\n",
    "#     )\n",
    "#     end_time = time.time()\n",
    "#     computation_time = end_time - start_time\n",
    "#     # print(sequences[0]['generated_text'])\n",
    "#     return sequences[0]['generated_text'], computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Batch processing of queries\n",
    "# # def process_queries_in_batches(df, batch_size=30):\n",
    "# #     results = []\n",
    "# #     for batch_start in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "# #         batch_end = min(batch_start + batch_size, len(df))\n",
    "# #         queries = df['SBERT Prompt'][batch_start:batch_end]\n",
    "# #         batch_results = [generate_response(query) for query in queries]\n",
    "# #         results.extend(batch_results)\n",
    "# #     return results\n",
    "\n",
    "# # Batch processing of queries\n",
    "# def process_queries_in_batches(listA, batch_size=10):\n",
    "#     results = []\n",
    "#     for batch_start in tqdm(range(0, len(listA), batch_size), desc=\"Processing batches\"):\n",
    "#         batch_end = min(batch_start + batch_size, len(df))\n",
    "#         queries = listA[batch_start:batch_end]\n",
    "#         batch_results = [generate_response(query) for query in queries]\n",
    "#         results.extend(batch_results)\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['SBERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Annotation Text'], row['SBERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "# df['BERT Prompt'] = df.apply(lambda row: generate_prompt_for_row(row['Annotation Text'], row['BERT_Ranking'].keys(), id_to_axiom_sentence), axis=1)\n",
    "\n",
    "# # Apply the function to each row in the DataFrame in batches and record results\n",
    "# batched_results = process_queries_in_batches(df['BERT Prompt'])\n",
    "# df['Answer_BERT_llama_7b'], df['Time_BERT_prompt_llama2_7b'] = zip(*batched_results)\n",
    "\n",
    "# sbert_batched_results = process_queries_in_batches(df['SBERT Prompt'])\n",
    "# df['Answer_SBERT_llama_7b'], df['Time_SBERT_prompt_llama2_7b'] = zip(*batched_results)\n",
    "\n",
    "# # Save the DataFrame\n",
    "# final_output_file = file_path+\"_llama.pkl\"\n",
    "# df.to_pickle(final_output_file)\n",
    "\n",
    "# # Display the first few rows of the DataFrame to verify\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
