{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec DataLabDisc in /data/milenadeswart/.local/share/jupyter/kernels/datalabdisc\n",
      "/data/anaconda3/bin/jupyter\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Package                            Version\n",
      "---------------------------------- -----------\n",
      "absl-py                            2.1.0\n",
      "aiohttp                            3.8.6\n",
      "aiosignal                          1.3.1\n",
      "alabaster                          0.7.12\n",
      "alembic                            1.6.5\n",
      "altair                             5.0.1\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 1.9.7\n",
      "anaconda-project                   0.8.3\n",
      "appdirs                            1.4.4\n",
      "asn1crypto                         1.0.1\n",
      "astroid                            2.3.1\n",
      "astropy                            3.2.2\n",
      "async-generator                    1.10\n",
      "async-timeout                      4.0.3\n",
      "asynctest                          0.13.0\n",
      "atomicwrites                       1.3.0\n",
      "attrs                              19.2.0\n",
      "Babel                              2.7.0\n",
      "backcall                           0.1.0\n",
      "backports.functools-lru-cache      1.5\n",
      "backports.os                       0.1.1\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "backports.zoneinfo                 0.2.1\n",
      "beautifulsoup4                     4.8.0\n",
      "bitarray                           1.0.1\n",
      "bkcharts                           0.2\n",
      "bleach                             3.1.0\n",
      "blinker                            1.6.3\n",
      "bokeh                              1.3.4\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.2.1\n",
      "cachetools                         5.3.3\n",
      "certifi                            2019.9.11\n",
      "cffi                               1.12.3\n",
      "chardet                            3.0.4\n",
      "charset-normalizer                 3.3.2\n",
      "click                              8.1.7\n",
      "cloudpickle                        1.2.2\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.1\n",
      "conda                              4.10.3\n",
      "conda-build                        3.18.9\n",
      "conda-package-handling             1.6.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0\n",
      "cryptography                       2.7\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.13\n",
      "cytoolz                            0.10.0\n",
      "dask                               2.5.2\n",
      "datasets                           2.13.2\n",
      "decorator                          4.4.0\n",
      "defusedxml                         0.6.0\n",
      "dill                               0.3.6\n",
      "distributed                        2.5.2\n",
      "docker-pycreds                     0.4.0\n",
      "docutils                           0.15.2\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "Flask                              1.1.1\n",
      "frozenlist                         1.3.3\n",
      "fsspec                             2023.1.0\n",
      "future                             0.17.1\n",
      "gevent                             1.4.0\n",
      "gitdb                              4.0.11\n",
      "GitPython                          3.1.43\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "google-auth                        2.29.0\n",
      "google-auth-oauthlib               0.4.6\n",
      "greenlet                           0.4.15\n",
      "grpcio                             1.62.2\n",
      "h5py                               2.9.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.0.1\n",
      "huggingface-hub                    0.16.4\n",
      "idna                               2.8\n",
      "imageio                            2.6.0\n",
      "imagesize                          1.1.0\n",
      "importlib-metadata                 6.7.0\n",
      "ipykernel                          5.1.2\n",
      "ipython                            7.8.0\n",
      "ipython_genutils                   0.2.0\n",
      "ipywidgets                         7.5.1\n",
      "isort                              4.3.21\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.15.1\n",
      "jeepney                            0.4.1\n",
      "Jinja2                             2.10.3\n",
      "joblib                             0.13.2\n",
      "json5                              0.8.5\n",
      "jsonschema                         3.0.2\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     5.3.3\n",
      "jupyter-console                    6.0.0\n",
      "jupyter-core                       4.5.0\n",
      "jupyterhub                         0.9.6\n",
      "jupyterlab                         1.1.4\n",
      "jupyterlab-server                  1.0.6\n",
      "keyring                            18.0.0\n",
      "kiwisolver                         1.1.0\n",
      "lazy-object-proxy                  1.4.2\n",
      "libarchive-c                       2.8\n",
      "lief                               0.9.0\n",
      "lime                               0.2.0.1\n",
      "llvmlite                           0.29.0\n",
      "locket                             0.2.0\n",
      "lxml                               4.4.1\n",
      "Mako                               1.1.4\n",
      "Markdown                           3.4.4\n",
      "markdown-it-py                     2.2.0\n",
      "MarkupSafe                         2.1.5\n",
      "matplotlib                         3.1.1\n",
      "mccabe                             0.6.1\n",
      "mdurl                              0.1.2\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.0.14\n",
      "mkl-random                         1.1.0\n",
      "mkl-service                        2.3.0\n",
      "mock                               3.0.5\n",
      "more-itertools                     7.2.0\n",
      "mpmath                             1.1.0\n",
      "msgpack                            0.6.1\n",
      "multidict                          6.0.5\n",
      "multipledispatch                   0.6.0\n",
      "multiprocess                       0.70.14\n",
      "navigator-updater                  0.2.1\n",
      "nbconvert                          5.6.0\n",
      "nbformat                           4.4.0\n",
      "networkx                           2.3\n",
      "nltk                               3.4.5\n",
      "nodejs                             0.1.1\n",
      "nose                               1.3.7\n",
      "notebook                           6.0.1\n",
      "numba                              0.45.1\n",
      "numexpr                            2.7.0\n",
      "numpy                              1.17.2\n",
      "numpydoc                           0.9.1\n",
      "nvidia-cublas-cu11                 11.10.3.66\n",
      "nvidia-cuda-nvrtc-cu11             11.7.99\n",
      "nvidia-cuda-runtime-cu11           11.7.99\n",
      "nvidia-cudnn-cu11                  8.5.0.96\n",
      "oauthlib                           3.2.2\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.0\n",
      "optional-django                    0.1.0\n",
      "packaging                          23.2\n",
      "pamela                             1.0.0\n",
      "pandas                             0.25.1\n",
      "pandocfilters                      1.4.2\n",
      "parso                              0.5.1\n",
      "partd                              1.0.0\n",
      "path.py                            12.0.1\n",
      "pathlib2                           2.3.5\n",
      "patsy                              0.5.1\n",
      "pep517                             0.13.1\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.7.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             6.2.0\n",
      "pip                                24.0\n",
      "pkginfo                            1.5.0.1\n",
      "pluggy                             0.13.0\n",
      "ply                                3.11\n",
      "prometheus-client                  0.7.1\n",
      "prompt-toolkit                     2.0.10\n",
      "protobuf                           3.20.3\n",
      "psutil                             5.6.3\n",
      "ptyprocess                         0.6.0\n",
      "py                                 1.8.0\n",
      "pyarrow                            12.0.1\n",
      "pyasn1                             0.5.1\n",
      "pyasn1-modules                     0.3.0\n",
      "pycodestyle                        2.5.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.19\n",
      "pycrypto                           2.6.1\n",
      "pycurl                             7.43.0.3\n",
      "pydeck                             0.8.1b1\n",
      "pyflakes                           2.1.1\n",
      "Pygments                           2.17.2\n",
      "pylint                             2.4.2\n",
      "Pympler                            1.0.1\n",
      "pyodbc                             4.0.27\n",
      "pyOpenSSL                          19.0.0\n",
      "pyparsing                          2.4.2\n",
      "PyQt5                              5.12.3\n",
      "PyQt5-Qt5                          5.15.2\n",
      "PyQt5-sip                          12.13.0\n",
      "PyQtWebEngine                      5.12.1\n",
      "PyQtWebEngine-Qt5                  5.15.2\n",
      "pyrsistent                         0.15.4\n",
      "PySocks                            1.7.1\n",
      "pytest                             5.2.1\n",
      "pytest-arraydiff                   0.3\n",
      "pytest-astropy                     0.5.0\n",
      "pytest-doctestplus                 0.4.0\n",
      "pytest-openfiles                   0.4.0\n",
      "pytest-remotedata                  0.3.2\n",
      "python-dateutil                    2.8.0\n",
      "python-editor                      1.0.4\n",
      "python-oauth2                      1.1.1\n",
      "pytz                               2019.3\n",
      "pytz-deprecation-shim              0.1.0.post0\n",
      "PyWavelets                         1.0.3\n",
      "PyYAML                             5.1.2\n",
      "pyzmq                              18.1.0\n",
      "QtAwesome                          0.6.0\n",
      "qtconsole                          4.5.5\n",
      "QtPy                               1.9.0\n",
      "regex                              2024.4.16\n",
      "requests                           2.22.0\n",
      "requests-oauthlib                  2.0.0\n",
      "rich                               13.7.1\n",
      "rope                               0.14.0\n",
      "rsa                                4.9\n",
      "ruamel_yaml                        0.15.46\n",
      "safetensors                        0.4.3\n",
      "scikit-image                       0.15.0\n",
      "scikit-learn                       0.21.3\n",
      "scipy                              1.3.1\n",
      "seaborn                            0.9.0\n",
      "SecretStorage                      3.1.1\n",
      "Send2Trash                         1.5.0\n",
      "sentencepiece                      0.2.0\n",
      "sentry-sdk                         1.9.0\n",
      "seqeval                            1.2.2\n",
      "setproctitle                       1.3.3\n",
      "setuptools                         68.0.0\n",
      "simplegeneric                      0.8.1\n",
      "simpletransformers                 0.63.11\n",
      "singledispatch                     3.4.0.3\n",
      "six                                1.12.0\n",
      "smmap                              5.0.1\n",
      "snowballstemmer                    2.0.0\n",
      "sortedcollections                  1.1.2\n",
      "sortedcontainers                   2.1.0\n",
      "soupsieve                          1.9.3\n",
      "Sphinx                             2.2.0\n",
      "sphinxcontrib-applehelp            1.0.1\n",
      "sphinxcontrib-devhelp              1.0.1\n",
      "sphinxcontrib-htmlhelp             1.0.2\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.2\n",
      "sphinxcontrib-serializinghtml      1.1.3\n",
      "sphinxcontrib-websupport           1.1.2\n",
      "spyder                             3.3.6\n",
      "spyder-kernels                     0.5.2\n",
      "SQLAlchemy                         1.3.9\n",
      "statsmodels                        0.10.1\n",
      "streamlit                          1.23.1\n",
      "sympy                              1.4\n",
      "tables                             3.5.2\n",
      "tblib                              1.4.0\n",
      "tenacity                           8.2.3\n",
      "tensorboard                        2.11.2\n",
      "tensorboard-data-server            0.6.1\n",
      "tensorboard-plugin-wit             1.8.1\n",
      "terminado                          0.8.2\n",
      "testpath                           0.4.2\n",
      "tokenizers                         0.13.3\n",
      "toml                               0.10.2\n",
      "tomli                              2.0.1\n",
      "toolz                              0.10.0\n",
      "torch                              1.13.1\n",
      "tornado                            6.0.3\n",
      "tqdl                               0.0.4\n",
      "tqdm                               4.66.2\n",
      "traitlets                          4.3.3\n",
      "transformers                       4.30.2\n",
      "typed-ast                          1.4.3\n",
      "typing                             3.7.4.3\n",
      "typing_extensions                  4.7.1\n",
      "tzdata                             2024.1\n",
      "tzlocal                            4.3.1\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.24.2\n",
      "validators                         0.20.0\n",
      "wandb                              0.16.6\n",
      "watchdog                           3.0.0\n",
      "wcwidth                            0.1.7\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           2.2.3\n",
      "wheel                              0.42.0\n",
      "widgetsnbextension                 3.5.1\n",
      "wrapt                              1.11.2\n",
      "wurlitzer                          1.0.3\n",
      "xlrd                               1.2.0\n",
      "XlsxWriter                         1.2.1\n",
      "xlwt                               1.3.0\n",
      "xxhash                             3.4.1\n",
      "yarl                               1.9.4\n",
      "zict                               1.0.0\n",
      "zipp                               0.6.0\n"
     ]
    }
   ],
   "source": [
    "# code aangepast van https://www.philschmid.de/bert-text-classification-in-a-different-language\n",
    "!ipython kernel install --user --name DataLabDisc\n",
    "!which jupyter\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pyarrow\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# check device waarop de code runt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# gebruik pretrained model (cased!): https://huggingface.co/GroNLP/bert-base-dutch-cased\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen & mergen data\n",
    "def merge_annotated_data(naam_dataset, n_annotators, n_annotations = 3):\n",
    "    \"\"\" functie die de gesplitste data gesplitst door split_annotated_data weer kan samenvoegen.\n",
    "    De datasets met de naam naam_dataset_n_annotators.csv worden ingeladen. Vervolgens wordt de oorspronkelijke dataset gereconstrueerd\n",
    "    uit de ingeladen data. De oorspronkelijke zinnen en het overzicht van de labels wordt gereturned.\n",
    "\n",
    "    Input:\n",
    "    naam_dataset: str, de naam van de documenen die de annotators hebben ingevuld\n",
    "    n_annotators: int, het aantal annotators\n",
    "    n_annotations: int, het aantal annotations per zin. default 3\n",
    "\n",
    "    Output:\n",
    "    merged_data: een DataFrame met daarin de oorspronkelijke data en de n_annotations labels die zijn toegekend.\n",
    "    \"\"\"\n",
    "    # inladen path naar data en creëren lege frames\n",
    "    dataset_path = \"~/Projecten/programma-discriminatie-en-racisme/datasets/Datasets_2/\"\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_annotators):\n",
    "        # inladen annotated documenten en samenvoegen\n",
    "        annotated_doc = pd.read_excel(dataset_path + naam_dataset + \"_\" + str(i) + '.xlsx')\n",
    "        annotated_doc = annotated_doc.dropna(subset=['Label', 'text', 'Column1'])\n",
    "        columns = list(annotated_doc.columns)\n",
    "\n",
    "        #display(annotated_doc)\n",
    "\n",
    "        # document opstellen\n",
    "        if i == 0:\n",
    "            merged_data = annotated_doc\n",
    "        else:\n",
    "            merged_data = pd.concat([merged_data, annotated_doc])\n",
    "\n",
    "    display(merged_data)\n",
    "\n",
    "    merged_data = merged_data.groupby(['Column1']).agg({'id': 'last', 'titel': 'last', 'text': 'last', 'zin': 'last', 'Label': 'mean'})\n",
    "    #merged_data = merged_data.sort_values(by = ['Column1'])\n",
    "\n",
    "    return pd.DataFrame(merged_data)\n",
    "\n",
    "merged = merge_annotated_data('te_labellen', n_annotators = 8, n_annotations = 3)\n",
    "\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bekijken dataset\n",
    "print(merged.columns)\n",
    "\n",
    "pos = len(merged[merged.Label == 1.0])\n",
    "neg = len(merged[merged.Label==0.0])\n",
    "unclear = len(merged[merged.Label==4.0])\n",
    "useless = len(merged[merged.Label==7.0])\n",
    "\n",
    "print(f\"The dataset has {pos} biased and {neg} unbiased instances for a total of {pos+neg}. There are {unclear} unclear labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1779 593 594\n"
     ]
    }
   ],
   "source": [
    "# prepareer dataset voor inladen in model\n",
    "merged = merged[merged.Label < 4.0] # behoud alleen labels 1 en 0\n",
    "\n",
    "merged = merged.drop(columns = ['id', 'titel', 'zin'])\n",
    "merged = merged.rename(columns={\"Label\": \"label\", 'text': 'text'}) # totaalscore/label = doel, text = ingelezen tekst\n",
    "\n",
    "# split dataset: 60% train, 20% val, 20% test\n",
    "train_val_df, test_df = train_test_split(merged, test_size=0.20)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25)\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at GroNLP/bert-base-dutch-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "# to change parameters: zie https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model\n",
    "train_args ={\"reprocess_input_data\": True, # tokenization gebeurt binnen model\n",
    "             \"fp16\":False, # vorm van de input van de data\n",
    "             \"silent\": False, # zeker weten dat progress bars worden geprint\n",
    "             \"use_multiprocessing\": False, # versnelt training\n",
    "             \"use_multiprocessing_for_evaluation\": False, # versnelt training\n",
    "             \"overwrite_output_dir\": False, # niet ieder model opslaan\n",
    "             \"save_total_limit\": 2, # slaat meest recente en beste epoch op\n",
    "             \"save_strategy\": 'no', # niet tussentijds opslaan\n",
    "             \"load_best_model_at_end\": False, # niet het beste model inladen (dat kan niet door opsla-instellingen)\n",
    "             \"num_train_epochs\": 5, # hoeveel epochs\n",
    "             \"evaluate_during_training\": True, # om validation data in te laten\n",
    "            \"evaluation_strategy\":\"steps\", # slaat een tussentijds model op om te evalueren\n",
    "            \"eval_steps\": 2000, # hoe vaak het model wordt opgeslagen\n",
    "            \"train_batch_size\": 24, # batch size trainingsdata\n",
    "            \"eval_batch_size\": 24} # batch size validation data\n",
    "\n",
    "# Create a ClassificationModel (code uit bron; zie bovenaan notebook)\n",
    "model = ClassificationModel(\n",
    "    \"bert\", \"GroNLP/bert-base-dutch-cased\", # Nederlands!!\n",
    "    #\"bert\", \"bert-base-uncased\", # English!!\n",
    "    num_labels=2, # hoeveel mogelijke targets er zijn\n",
    "    args=train_args,\n",
    "    use_cuda=False # omdat dit op de cpu draait\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a80092bc174256a558fb8e827a2d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ec2f4b80a4eb3bf9a54d64343a231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a06be05597e443fa30d037295d71eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe03028f98b48c68ca6553782ee15c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27db94b702514bbfa50f18188627e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a86434df6f143df815da8b37f93448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 5:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainen!\n",
    "train_results = model.train_model(train_df = train_df, eval_df = val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa01ec6461304b609955bfbf4cc4241f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testen\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# om f1 te kunnen genereren\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='macro')\n",
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df, f1=f1_multiclass, acc=accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mcc': 0.3804325375972074, 'tp': 33, 'tn': 476, 'fp': 22, 'fn': 63, 'auroc': 0.8051999665327978, 'auprc': 0.48399945806103417, 'f1': 0.6775594398002389, 'acc': 0.8569023569023569, 'eval_loss': 0.7593391788750887}\n"
     ]
    }
   ],
   "source": [
    "# resultaten: mcc = , tp = true positives, tn = true negatives, fp = false negatives, fn = false negatives\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# maak een plotje van de loss\n",
    "train_losses = train_results[1]['train_loss']\n",
    "eval_losses = train_results[1]['eval_loss']\n",
    "\n",
    "plt.plot(train_losses, c = 'teal', label = 'train loss')\n",
    "plt.plot(eval_losses, c = 'deeppink', label = 'val loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and evaluation loss over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, defaultdict(<class 'list'>, {'global_step': [75, 150, 225, 300, 375], 'train_loss': [0.055035557597875595, 1.1680638790130615, 0.2238083928823471, 0.00414998410269618, 0.003763132030144334], 'mcc': [0.3464477196669204, 0.402463414892708, 0.4052040738659553, 0.3611392055217786, 0.35981091043115754], 'tp': [24, 35, 46, 38, 30], 'tn': [480, 472, 453, 459, 473], 'fp': [10, 18, 37, 31, 17], 'fn': [79, 68, 57, 65, 73], 'auroc': [0.7738953833960769, 0.8005250643946898, 0.7824351099663166, 0.7635724192589657, 0.7550326926887261], 'auprc': [0.5293161772882675, 0.5729402223493186, 0.5508289447368235, 0.5348510967303476, 0.5323360100448771], 'eval_loss': [0.39917704641819, 0.39893104195594786, 0.5192696335166693, 0.678456740155816, 0.7484239959204569]}))\n"
     ]
    }
   ],
   "source": [
    "print(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(eval_df = val_df) #, output_dir = output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1521, 15800,   117, 10031, 27707,   117,   131, 22482,   124,\n",
      "         13177, 20049, 11281, 29448, 28236, 27106, 10031, 18070, 29167, 26208,\n",
      "         17586, 29265,  8470, 20271, 25262,   115, 13912,   125,   131, 13644,\n",
      "         20125,   135,   126,   131,  8470, 10156, 28999,    13,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ClassificationModel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5e0e158f8685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    413\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[1;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             distance_metric=distance_metric)\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[0;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ClassificationModel' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class_names = ['0', '1']\n",
    "\n",
    "text = 'Building more bypasses will help the environment by reducing pollution and traffic jams in towns and cities.'\n",
    "print(tokenizer(text, return_tensors='pt', padding=True))\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(text, model, num_features=20, num_samples=2000)\n",
    "exp.show_in_notebook(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUDE/ANDERE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset_zoek.drop(columns = ['Unnamed: 0', 'document', 'zin', 'label', 'score_annotator'])\n",
    "#dataset = dataset.rename(columns={\"totaalscore\": \"label\", 'text': 'text'})\n",
    "print(dataset.columns)\n",
    "\n",
    "\n",
    "X = list(dataset_zoek[\"text\"])\n",
    "y = list(dataset_zoek[\"totaalscore\"])\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.25)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_train = tokenizer(dataset['train'][\"text\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "#tokenized_data = dict(tokenized_train)\n",
    "                     \n",
    "#labels_train = np.array(dataset['train'][\"label\"])  # Label is already an array of 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainen\n",
    "# sla training_arguments op\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    "    reprocess_input_data= True,\n",
    "    fp16=False,\n",
    "    overwrite_output_dir = True\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model!\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maak dataset correcte format: [{'label': value, 'text': text}, {'label': value, 'text': text}]\n",
    "data = []\n",
    "\n",
    "for i in range(len(dataset_zoek)):\n",
    "    label = dataset_zoek['totaalscore'].iloc[i]\n",
    "    text = dataset_zoek['zin'].iloc[i]\n",
    "    tokenized_text = tokenizer(str(text), return_tensors=\"np\", padding=True)\n",
    "    duo = {'label': label, 'text': tokenized_text}\n",
    "    data.append(duo)\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset: 60% train, 20% val, 20% test\n",
    "trainval, test = train_test_split(data, train_size = 0.8)\n",
    "train, val = train_test_split(trainval, train_size = 0.75)\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "\n",
    "for item in train:\n",
    "    train_labels.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Read data\n",
    "dataset_zoek = pd.read_csv(dataset_path + 'dataset_dummy.csv', sep = ',') # model is te groot om te trainen met volledige dataset zonder GPU\n",
    "dataset = dataset_zoek.drop(columns = ['Unnamed: 0', 'document', 'zin', 'label', 'score_annotator'])\n",
    "dataset = dataset.rename(columns={\"totaalscore\": \"label\", 'text': 'text'})\n",
    "\n",
    "# Define pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
    "model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")  # PyTorch\n",
    "\n",
    "# ----- 1. Preprocess data -----#\n",
    "# Preprocess data\n",
    "X = list(dataset_zoek[\"text\"])\n",
    "y = list(dataset_zoek[\"totaalscore\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. Predict -----#\n",
    "# Load test data\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "X_test = list(test_data[\"review\"])\n",
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Create torch dataset\n",
    "test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"output/checkpoint-50000\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Define test trainer\n",
    "test_trainer = Trainer(model)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "y_pred = np.argmax(raw_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(dataset)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# split dataset: 60% train, 20% val, 20% test\n",
    "train_testvalid = ds.train_test_split(test_size=0.4)\n",
    "# Split the 40% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "print(ds['train']['text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "        tokenizer=None, \n",
    "        topic=None, \n",
    "        freeze_bert = None, \n",
    "        num_feats=None, \n",
    "        keep_stopwords = None,\n",
    "        combined_feats = None, \n",
    "        sum_layers = None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "       \n",
    "        combined_feats=aggregate_features(outputs, tokenizer, input_ids, topic, \n",
    "                                     num_feats, keep_stopwords, sum_layers).to(device)\n",
    "        combined_feats = self.dropout(combined_feats)\n",
    "        logits = self.classifier(combined_feats)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataLabDisc",
   "language": "python",
   "name": "datalabdisc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
